{
  "reward_only": {
    "correlations": {
      "combined_score": {
        "pearson_r": 0.6008406760158833,
        "pearson_p": 3.875297964307653e-11,
        "spearman_r": 0.5776613892849563,
        "spearman_p": 3.1283547300641385e-10,
        "mean_abs_diff": 0.109985,
        "claude_mean": 0.7712150000000001,
        "gemini_mean": 0.8550000000000001,
        "claude_std": 0.10740198450214969,
        "gemini_std": 0.11697275323766643
      },
      "ahimsa_score": {
        "pearson_r": 0.39019884720679043,
        "pearson_p": 5.989507345832046e-05,
        "spearman_r": 0.12874954922558568,
        "spearman_p": 0.20173321061158322,
        "mean_abs_diff": 0.15108333333333332,
        "claude_mean": 0.7855,
        "gemini_mean": 0.9142500000000001,
        "claude_std": 0.11910907699341064,
        "gemini_std": 0.12924081205254012
      },
      "dharma_score": {
        "pearson_r": 0.5926356055950408,
        "pearson_p": 8.272131514543196e-11,
        "spearman_r": 0.510571477010312,
        "spearman_p": 5.731906333067593e-08,
        "mean_abs_diff": 0.0707,
        "claude_mean": 0.9386,
        "gemini_mean": 0.9105,
        "claude_std": 0.19692648374456903,
        "gemini_std": 0.25478373181975333
      },
      "helpfulness_score": {
        "pearson_r": 0.6961354568076644,
        "pearson_p": 8.972391740427791e-16,
        "spearman_r": 0.7148627217086109,
        "spearman_p": 6.634658258687967e-17,
        "mean_abs_diff": 0.19549999999999998,
        "claude_mean": 0.53375,
        "gemini_mean": 0.72175,
        "claude_std": 0.15416610360257538,
        "gemini_std": 0.15663073612800266
      }
    },
    "bias_analysis": {
      "combined_score": {
        "mean_difference": -0.08378500000000001,
        "std_difference": 0.10060298839994763,
        "median_difference": -0.09000000000000008,
        "min_difference": -0.5050000000000001,
        "max_difference": 0.29499999999999993,
        "t_statistic": -8.286535394610759,
        "t_p_value": 5.817122112218807e-13,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      },
      "ahimsa_score": {
        "mean_difference": -0.12874999999999998,
        "std_difference": 0.13739326665532856,
        "median_difference": -0.1499999999999999,
        "min_difference": -0.65,
        "max_difference": 0.4,
        "t_statistic": -9.323938184601637,
        "t_p_value": 3.264446189840133e-15,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      },
      "dharma_score": {
        "mean_difference": 0.028100000000000003,
        "std_difference": 0.21029833570430365,
        "median_difference": 0.0,
        "min_difference": -1.0,
        "max_difference": 1.0,
        "t_statistic": 1.3294992035509416,
        "t_p_value": 0.1867387618050543,
        "significant_bias": false,
        "bias_direction": "claude_higher"
      },
      "helpfulness_score": {
        "mean_difference": -0.188,
        "std_difference": 0.12116517651536682,
        "median_difference": -0.17500000000000004,
        "min_difference": -0.5249999999999999,
        "max_difference": 0.19999999999999996,
        "t_statistic": -15.438234281144377,
        "t_p_value": 4.181535465913918e-28,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      }
    },
    "ranking_analysis": {
      "combined_score": {
        "rank_correlation": 0.5776613892849563,
        "rank_p_value": 3.1283547300641385e-10,
        "mean_rank_difference": 18.91,
        "max_rank_difference": 95.0,
        "agreement_strength": "moderate"
      },
      "ahimsa_score": {
        "rank_correlation": 0.12874954922558568,
        "rank_p_value": 0.20173321061158322,
        "mean_rank_difference": 29.95,
        "max_rank_difference": 85.0,
        "agreement_strength": "weak"
      },
      "dharma_score": {
        "rank_correlation": 0.510571477010312,
        "rank_p_value": 5.731906333067593e-08,
        "mean_rank_difference": 9.05,
        "max_rank_difference": 57.5,
        "agreement_strength": "moderate"
      },
      "helpfulness_score": {
        "rank_correlation": 0.7148627217086109,
        "rank_p_value": 6.634658258687967e-17,
        "mean_rank_difference": 16.02,
        "max_rank_difference": 67.0,
        "agreement_strength": "strong"
      }
    },
    "summary_comparison": {
      "claude_combined": 0.7712150000000001,
      "gemini_combined": 0.8550000000000004,
      "summary_difference": -0.08378500000000033
    }
  },
  "policy_only": {
    "correlations": {
      "combined_score": {
        "pearson_r": 0.49918587856390656,
        "pearson_p": 1.2467789732368117e-07,
        "spearman_r": 0.4817038266894951,
        "spearman_p": 3.89679800332221e-07,
        "mean_abs_diff": 0.12578999999999999,
        "claude_mean": 0.7365300000000002,
        "gemini_mean": 0.7828999999999999,
        "claude_std": 0.11337309689692701,
        "gemini_std": 0.16464191446894683
      },
      "ahimsa_score": {
        "pearson_r": 0.38523763081133394,
        "pearson_p": 7.563291813174218e-05,
        "spearman_r": 0.29458392288319907,
        "spearman_p": 0.0029287595980780233,
        "mean_abs_diff": 0.129,
        "claude_mean": 0.7948333333333332,
        "gemini_mean": 0.8710000000000001,
        "claude_std": 0.11981919248972134,
        "gemini_std": 0.15057058145600688
      },
      "dharma_score": {
        "pearson_r": 0.497917878397269,
        "pearson_p": 1.3571284690827744e-07,
        "spearman_r": 0.5336585290844649,
        "spearman_p": 1.0835585976104814e-08,
        "mean_abs_diff": 0.1858,
        "claude_mean": 0.8341999999999998,
        "gemini_mean": 0.767,
        "claude_std": 0.2800542090381789,
        "gemini_std": 0.36967959099739334
      },
      "helpfulness_score": {
        "pearson_r": 0.5131759891614598,
        "pearson_p": 4.7791584232521504e-08,
        "spearman_r": 0.4457425771140193,
        "spearman_p": 3.360506195105143e-06,
        "mean_abs_diff": 0.1905,
        "claude_mean": 0.548,
        "gemini_mean": 0.716,
        "claude_std": 0.13056033088193367,
        "gemini_std": 0.15765944310443317
      }
    },
    "bias_analysis": {
      "combined_score": {
        "mean_difference": -0.046369999999999995,
        "std_difference": 0.14603029856848201,
        "median_difference": -0.08249999999999996,
        "min_difference": -0.4794999999999999,
        "max_difference": 0.36249999999999993,
        "t_statistic": -3.1594516967310997,
        "t_p_value": 0.0020967822834997182,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      },
      "ahimsa_score": {
        "mean_difference": -0.07616666666666663,
        "std_difference": 0.15207828758753025,
        "median_difference": -0.09999999999999998,
        "min_difference": -0.7166666666666666,
        "max_difference": 0.475,
        "t_statistic": -4.98328049729006,
        "t_p_value": 2.659000537286933e-06,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      },
      "dharma_score": {
        "mean_difference": 0.0672,
        "std_difference": 0.3346552853310403,
        "median_difference": 0.0,
        "min_difference": -0.88,
        "max_difference": 0.97,
        "t_statistic": 1.9979710079111397,
        "t_p_value": 0.04846298773256586,
        "significant_bias": true,
        "bias_direction": "claude_higher"
      },
      "helpfulness_score": {
        "mean_difference": -0.168,
        "std_difference": 0.14413882197381803,
        "median_difference": -0.1875,
        "min_difference": -0.6000000000000001,
        "max_difference": 0.175,
        "t_statistic": -11.59700677061697,
        "t_p_value": 3.752439334247919e-20,
        "significant_bias": true,
        "bias_direction": "gemini_higher"
      }
    },
    "ranking_analysis": {
      "combined_score": {
        "rank_correlation": 0.4817038266894951,
        "rank_p_value": 3.89679800332221e-07,
        "mean_rank_difference": 22.36,
        "max_rank_difference": 93.0,
        "agreement_strength": "weak"
      },
      "ahimsa_score": {
        "rank_correlation": 0.29458392288319907,
        "rank_p_value": 0.0029287595980780233,
        "mean_rank_difference": 26.68,
        "max_rank_difference": 85.5,
        "agreement_strength": "weak"
      },
      "dharma_score": {
        "rank_correlation": 0.5336585290844649,
        "rank_p_value": 1.0835585976104814e-08,
        "mean_rank_difference": 16.89,
        "max_rank_difference": 68.0,
        "agreement_strength": "moderate"
      },
      "helpfulness_score": {
        "rank_correlation": 0.4457425771140193,
        "rank_p_value": 3.360506195105143e-06,
        "mean_rank_difference": 22.94,
        "max_rank_difference": 89.0,
        "agreement_strength": "weak"
      }
    },
    "summary_comparison": {
      "claude_combined": 0.73653,
      "gemini_combined": 0.7828999999999999,
      "summary_difference": -0.04636999999999991
    }
  }
}