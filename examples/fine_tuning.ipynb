{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArGen GRPO Fine-Tuning Example\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Large Language Model (LLM) using Group Relative Policy Optimization (GRPO) with Dharmic ethical principles (Ahimsa, Satya, Dharma) on the Predibase platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -e ..\n",
    "!pip install predibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import json\n",
    "import predibase as pb\n",
    "from src.reward_functions import ahimsa_reward, satya_reward, dharma_reward\n",
    "from src.predibase import create_grpo_config, submit_grpo_job\n",
    "from src.data_utils.dataset import load_jsonl_dataset, prepare_dataset_for_predibase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Authenticate with Predibase using your API key. You can get your API key from the Predibase platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Predibase API key\n",
    "# You can also set this as an environment variable: export PREDIBASE_API_KEY=your_api_key\n",
    "os.environ[\"PREDIBASE_API_KEY\"] = \"your_api_key_here\"\n",
    "\n",
    "# Initialize the Predibase client\n",
    "pb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "\n",
    "Load the healthcare dataset and prepare it for Predibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"../data/healthcare_examples.jsonl\"\n",
    "examples = load_jsonl_dataset(dataset_path)\n",
    "\n",
    "# Display a sample\n",
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for Predibase\n",
    "prepared_dataset_path = \"../data/healthcare_examples_prepared.jsonl\"\n",
    "prepare_dataset_for_predibase(\n",
    "    examples=examples,\n",
    "    output_path=prepared_dataset_path,\n",
    "    prompt_field=\"prompt\",\n",
    "    context_fields=[\"role\", \"patient_context\"]\n",
    ")\n",
    "\n",
    "# Upload the dataset to Predibase\n",
    "dataset = pb.datasets.create(\n",
    "    name=\"healthcare_examples\",\n",
    "    source=prepared_dataset_path,\n",
    "    description=\"Healthcare examples for ArGen GRPO fine-tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Reward Functions\n",
    "\n",
    "We'll use the pre-defined reward functions based on Dharmic principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward functions\n",
    "reward_functions = {\n",
    "    \"ahimsa\": ahimsa_reward,\n",
    "    \"satya\": satya_reward,\n",
    "    \"dharma\": dharma_reward\n",
    "}\n",
    "\n",
    "# Examine the reward functions\n",
    "print(ahimsa_reward.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Submit GRPO Fine-Tuning Job\n",
    "\n",
    "Configure the GRPO fine-tuning job and submit it to Predibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GRPO configuration\n",
    "grpo_config = create_grpo_config(\n",
    "    base_model=\"microsoft/phi-3-mini-4k-instruct\",\n",
    "    reward_functions=reward_functions,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=3,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the GRPO fine-tuning job\n",
    "job_id = submit_grpo_job(\n",
    "    config=grpo_config,\n",
    "    dataset=\"healthcare_examples\",\n",
    "    repo=\"argen-healthcare\",\n",
    "    description=\"ArGen GRPO fine-tuning with Dharmic principles for healthcare\"\n",
    ")\n",
    "\n",
    "print(f\"GRPO fine-tuning job submitted with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training Progress\n",
    "\n",
    "Monitor the progress of the GRPO fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the adapter\n",
    "adapter = pb.adapters.get(job_id)\n",
    "\n",
    "# Check the status\n",
    "print(f\"Status: {adapter.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-Tuned Model\n",
    "\n",
    "Once the fine-tuning job is complete, test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to complete\n",
    "# This may take a while, so you might want to check the status on the Predibase platform\n",
    "import time\n",
    "\n",
    "while adapter.status not in [\"COMPLETED\", \"FAILED\", \"CANCELLED\"]:\n",
    "    print(f\"Status: {adapter.status}\")\n",
    "    time.sleep(60)  # Check every minute\n",
    "    adapter = pb.adapters.get(job_id)\n",
    "\n",
    "print(f\"Final status: {adapter.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the job completed successfully, test the fine-tuned model\n",
    "if adapter.status == \"COMPLETED\":\n",
    "    # Create a deployment\n",
    "    deployment = pb.deployments.create(\n",
    "        name=\"argen-healthcare-deployment\",\n",
    "        adapter=job_id,\n",
    "        description=\"ArGen healthcare deployment\"\n",
    "    )\n",
    "    \n",
    "    # Test the model\n",
    "    test_prompt = \"I have a headache that won't go away after 3 days. What should I do?\"\n",
    "    \n",
    "    response = deployment.generate(\n",
    "        prompt=test_prompt,\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Base Model\n",
    "\n",
    "Compare the fine-tuned model with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment for the base model\n",
    "base_deployment = pb.deployments.create(\n",
    "    name=\"phi-3-mini-base\",\n",
    "    model=\"microsoft/phi-3-mini-4k-instruct\",\n",
    "    description=\"Base Phi-3 Mini model\"\n",
    ")\n",
    "\n",
    "# Test the base model\n",
    "base_response = base_deployment.generate(\n",
    "    prompt=test_prompt,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Base Model Response: {base_response.text}\")\n",
    "print(f\"Fine-tuned Model Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Reward Scores\n",
    "\n",
    "Evaluate the reward scores for both the base model and the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reward scores\n",
    "def evaluate_rewards(prompt, completion, example={}):\n",
    "    # Add default values for example\n",
    "    if \"role\" not in example:\n",
    "        example[\"role\"] = \"healthcare_assistant\"\n",
    "    if \"patient_context\" not in example:\n",
    "        example[\"patient_context\"] = \"Adult with persistent headache\"\n",
    "    \n",
    "    # Calculate rewards\n",
    "    ahimsa_score = ahimsa_reward(prompt, completion, example)\n",
    "    satya_score = satya_reward(prompt, completion, example)\n",
    "    dharma_score = dharma_reward(prompt, completion, example)\n",
    "    \n",
    "    # Calculate total score\n",
    "    total_score = (ahimsa_score + satya_score + dharma_score) / 3\n",
    "    \n",
    "    return {\n",
    "        \"ahimsa\": ahimsa_score,\n",
    "        \"satya\": satya_score,\n",
    "        \"dharma\": dharma_score,\n",
    "        \"total\": total_score\n",
    "    }\n",
    "\n",
    "# Evaluate base model\n",
    "base_scores = evaluate_rewards(test_prompt, base_response.text)\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "ft_scores = evaluate_rewards(test_prompt, response.text)\n",
    "\n",
    "# Display scores\n",
    "print(\"Base Model Scores:\")\n",
    "for key, value in base_scores.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFine-tuned Model Scores:\")\n",
    "for key, value in ft_scores.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to fine-tune a Large Language Model using GRPO with Dharmic ethical principles (Ahimsa, Satya, Dharma) on the Predibase platform. We showed how to:\n",
    "\n",
    "1. Prepare a healthcare dataset for fine-tuning\n",
    "2. Define reward functions based on Dharmic principles\n",
    "3. Configure and submit a GRPO fine-tuning job\n",
    "4. Monitor the training progress\n",
    "5. Test and evaluate the fine-tuned model\n",
    "6. Compare the fine-tuned model with the base model\n",
    "\n",
    "The fine-tuned model should demonstrate improved alignment with Dharmic ethical principles in a healthcare context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
